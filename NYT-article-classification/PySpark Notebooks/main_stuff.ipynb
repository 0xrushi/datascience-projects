{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NYC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "sc = SparkContext('local')\n",
    "line=sc.wholeTextFiles(path=\"../NYT articles/\")\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "import re\n",
    "\n",
    "a=line.collect()\n",
    "for i in range(len(a)):\n",
    "    a[i]=list(a[i])\n",
    "    a[i][0]=re.sub(r'\\d+', '',a[i][0])\n",
    "    a[i]=tuple(a[i])\n",
    "rdd = sc.parallelize(a)\n",
    "people = rdd.map(lambda x: Row(label=x[0], sentence=(x[1])))\n",
    "sqlContext = SQLContext(sc)\n",
    "schemaPeople = sqlContext.createDataFrame(people)\n",
    "regexTokenizer = RegexTokenizer(inputCol= 'sentence', outputCol='words')\n",
    "regexTokenized = regexTokenizer.transform(schemaPeople)\n",
    "#regexTokenized.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: string, sentence: string]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schemaPeople"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data is read using wholeTextFiles. This function can be used to read all the files present in the given directory. It returns a list of tuples. Each tuple contains address of file and data. Data is cleaned and seperated by tokenizer. Stopwords, digits and symbols are removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|               label|            sentence|               words|            filtered|         rawFeatures|            features|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|file:/home/hxd/Do...|AdvertisementSupp...|[advertisementsup...|[advertisementsup...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|\n",
      "|file:/home/hxd/Do...|AdvertisementSupp...|[advertisementsup...|[advertisementsup...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|\n",
      "|file:/home/hxd/Do...|AdvertisementSupp...|[advertisementsup...|[advertisementsup...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|\n",
      "|file:/home/hxd/Do...|AdvertisementSupp...|[advertisementsup...|[advertisementsup...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|\n",
      "|file:/home/hxd/Do...|AdvertisementSupp...|[advertisementsup...|[advertisementsup...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|\n",
      "|file:/home/hxd/Do...|AdvertisementSupp...|[advertisementsup...|[advertisementsup...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|\n",
      "|file:/home/hxd/Do...|AdvertisementSupp...|[advertisementsup...|[advertisementsup...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|\n",
      "|file:/home/hxd/Do...|AdvertisementSupp...|[advertisementsup...|[advertisementsup...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|\n",
      "|file:/home/hxd/Do...|AdvertisementSupp...|[advertisementsup...|[advertisementsup...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|\n",
      "|file:/home/hxd/Do...|AdvertisementSupp...|[advertisementsup...|[advertisementsup...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "sw=remover.transform(regexTokenized)\n",
    "\n",
    "\n",
    "# tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "# wordsData = tokenizer.transform(sentenceData)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(sw)\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "rescaledData.show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF is applied on the clean data which helps to get features for the words present in the file. It returns importance of every word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|   label|            features|\n",
      "+--------+--------------------+\n",
      "|baseball|(20,[0,1,2,3,4,5,...|\n",
      "|baseball|(20,[0,1,2,3,4,5,...|\n",
      "+--------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[label: string, features: vector]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "import re\n",
    "from pyspark.sql.types import *\n",
    "name='label'\n",
    "# udf = UserDefinedFunction(lambda x: re.sub('file:/home/shreyas/Lab3data/try/','',str(x)), StringType())\n",
    "# new_df = rescaledData.select(*[udf(column).alias(name) if column == name else column for column in rescaledData.columns])\n",
    "udf = UserDefinedFunction(lambda x: re.sub('.txt','',str(x)), StringType())\n",
    "new_df = rescaledData.select(*[udf(column).alias(name) if column == name else column for column in rescaledData.columns])\n",
    "# truncate preceding file part\n",
    "udf1 = UserDefinedFunction(lambda x: re.sub('file:/home/hxd/Documents/portfolioprojects/NYT-article-classification/NYT articles/','',str(x)), StringType())\n",
    "re = new_df.select(*[udf1(column).alias(name) if column == name else column for column in new_df.columns])\n",
    "\n",
    "re.select(\"label\",\"features\").show(2)\n",
    "final=re.select(\"label\", \"features\")\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|label   |\n",
      "+--------+\n",
      "|baseball|\n",
      "|baseball|\n",
      "+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Printing only labels\n",
    "re.select(\"label\").show(2, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The filename returned by wholeTextFile is used as label. It returns the absolute address of file. So the absolute address is removed and only label part is kept.(We have saved files with the labelname.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide data in train and test set\n",
    "\n",
    "###### Cleaned data is divided into train and test set by the ratio of 4:1\n",
    "\n",
    "## Convert string labels to integer\n",
    "\n",
    "##### Every label has been assigned by a integer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-------------+\n",
      "|   label|            features|categoryIndex|\n",
      "+--------+--------------------+-------------+\n",
      "|baseball|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|baseball|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|     NBA|(20,[0,1,2,3,4,5,...|          1.0|\n",
      "|baseball|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|     NBA|(20,[0,1,2,3,4,5,...|          1.0|\n",
      "|     NBA|(20,[0,1,2,3,4,5,...|          1.0|\n",
      "|     NBA|(20,[0,1,2,3,4,5,...|          1.0|\n",
      "|baseball|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|     NBA|(20,[0,1,2,3,4,5,...|          1.0|\n",
      "|baseball|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|baseball|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|     NBA|(20,[0,1,2,3,4,5,...|          1.0|\n",
      "|baseball|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|baseball|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|baseball|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|baseball|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|baseball|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|     NBA|(20,[0,1,2,3,4,5,...|          1.0|\n",
      "|baseball|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|baseball|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "+--------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import IndexToString\n",
    "\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"label\", outputCol=\"categoryIndex\")\n",
    "#indexed = labelIndexer.fit(final).transform(final)\n",
    "#indexed.show()\n",
    "labelIndexer= stringIndexer.fit(final)\n",
    "indexed= labelIndexer.transform(final)\n",
    "indexed.show()\n",
    "\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)\n",
    "\n",
    "\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(indexed)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(train, test) = indexed.randomSplit([0.8, 0.2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: string, features: vector, categoryIndex: double]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression using pipelining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-------------+\n",
      "|   label|            features|categoryIndex|\n",
      "+--------+--------------------+-------------+\n",
      "|baseball|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|baseball|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|     NBA|(20,[0,1,2,3,4,5,...|          1.0|\n",
      "|baseball|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|     NBA|(20,[0,1,2,3,4,5,...|          1.0|\n",
      "|     NBA|(20,[0,1,2,3,4,5,...|          1.0|\n",
      "|     NBA|(20,[0,1,2,3,4,5,...|          1.0|\n",
      "|baseball|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|     NBA|(20,[0,1,2,3,4,5,...|          1.0|\n",
      "|baseball|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|baseball|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|     NBA|(20,[0,1,2,3,4,5,...|          1.0|\n",
      "|baseball|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|baseball|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|baseball|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|baseball|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|baseball|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|     NBA|(20,[0,1,2,3,4,5,...|          1.0|\n",
      "|baseball|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "|baseball|(20,[0,1,2,3,4,5,...|          0.0|\n",
      "+--------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Coefficients: \n",
      "1 X 20 CSRMatrix\n",
      "(0,7) -1.3992\n",
      "Intercept: [-0.6085824431794001]\n",
      "+----------+--------------------+--------------+\n",
      "|prediction|            features|predictedLabel|\n",
      "+----------+--------------------+--------------+\n",
      "|       0.0|(20,[0,1,2,3,4,5,...|      baseball|\n",
      "|       0.0|(20,[0,1,2,3,4,5,...|      baseball|\n",
      "|       0.0|(20,[0,1,2,3,4,5,...|      baseball|\n",
      "|       0.0|(20,[0,1,2,3,4,5,...|      baseball|\n",
      "|       0.0|(20,[0,1,2,3,4,5,...|      baseball|\n",
      "|       0.0|(20,[0,1,2,3,4,5,...|      baseball|\n",
      "|       0.0|(20,[0,1,2,3,4,5,...|      baseball|\n",
      "|       0.0|(20,[0,1,2,3,4,5,...|      baseball|\n",
      "|       0.0|(20,[0,1,2,3,4,5,...|      baseball|\n",
      "|       0.0|(20,[0,1,2,3,4,5,...|      baseball|\n",
      "+----------+--------------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Test Error = 0.320755 \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "indexed.show()\n",
    "lr = LogisticRegression(labelCol=\"categoryIndex\", featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "# Train model with Training Data\n",
    "lrModel = lr.fit(train)\n",
    "print(\"Coefficients: \\n\" + str(lrModel.coefficientMatrix))\n",
    "print(\"Intercept: \" + str(lrModel.interceptVector))\n",
    "\n",
    "\n",
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)\n",
    "\n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipelinelr = Pipeline(stages=[labelIndexer, featureIndexer, lr, labelConverter])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "\n",
    "train2 = train.drop(col(\"categoryIndex\"))\n",
    "lrModel = pipelinelr.fit(train2)\n",
    "\n",
    "test2 = test.drop(col(\"categoryIndex\"))\n",
    "predictions =lrModel.transform(test2)\n",
    "\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"features\",\"predictedLabel\").show(10)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"categoryIndex\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+--------------------+\n",
      "|predictedLabel|label|            features|\n",
      "+--------------+-----+--------------------+\n",
      "|      baseball|  NBA|(20,[0,1,2,3,4,5,...|\n",
      "|      baseball|  NBA|(20,[0,1,2,3,4,5,...|\n",
      "|      baseball|  NBA|(20,[0,1,2,3,4,5,...|\n",
      "|      baseball|  NBA|(20,[0,1,2,3,4,5,...|\n",
      "|      baseball|  NBA|(20,[0,1,2,3,4,5,...|\n",
      "+--------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0.320755\n",
      "RandomForestClassificationModel: uid=RandomForestClassifier_9e9cdee78469, numTrees=10, numClasses=2, numFeatures=20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(final)\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(final)\n",
    "\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=10)\n",
    "\n",
    "# Convert indexed labels back to original labels.\n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipelinerf = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "rfmodel = pipelinerf.fit(train)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = rfmodel.transform(test)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "rfModel = rfmodel.stages[2]\n",
    "print(rfModel)  # summary only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+\n",
      "|   label|predictedLabel|\n",
      "+--------+--------------+\n",
      "|     NBA|      baseball|\n",
      "|     NBA|      baseball|\n",
      "|     NBA|      baseball|\n",
      "|     NBA|      baseball|\n",
      "|     NBA|      baseball|\n",
      "|     NBA|      baseball|\n",
      "|     NBA|      baseball|\n",
      "|     NBA|      baseball|\n",
      "|     NBA|      baseball|\n",
      "|     NBA|      baseball|\n",
      "|     NBA|      baseball|\n",
      "|     NBA|      baseball|\n",
      "|     NBA|      baseball|\n",
      "|     NBA|      baseball|\n",
      "|     NBA|      baseball|\n",
      "|     NBA|      baseball|\n",
      "|     NBA|      baseball|\n",
      "|baseball|      baseball|\n",
      "|baseball|      baseball|\n",
      "|baseball|      baseball|\n",
      "+--------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Test set accuracy = 0.6792452830188679\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "#import LabeledPoint\n",
    "\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\",labelCol=\"categoryIndex\")\n",
    "\n",
    "# train the model\n",
    "\n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipelinenb = Pipeline(stages=[labelIndexer, featureIndexer, nb, labelConverter])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "nbmodel = pipelinenb.fit(train)\n",
    "\n",
    "\n",
    "# select example rows to display.\n",
    "predictions = nbmodel.transform(test)\n",
    "predictions.select(\"label\",\"predictedLabel\").show()\n",
    "\n",
    "# compute accuracy on the test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"categoryIndex\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
